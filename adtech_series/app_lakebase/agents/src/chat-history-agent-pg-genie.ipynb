{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cf63ecd-9099-40b4-aa3a-0700f2576e5f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install Deps"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: databricks-sdk==0.61.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (0.61.0)\n",
      "Requirement already satisfied: databricks-connect==16.4.2 in /databricks/python3/lib/python3.12/site-packages (16.4.2)\n",
      "Requirement already satisfied: pyarrow<20 in /databricks/python3/lib/python3.12/site-packages (15.0.2)\n",
      "Requirement already satisfied: databricks-agents==1.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (1.4.0)\n",
      "Requirement already satisfied: mlflow<=3.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (3.2.0)\n",
      "Requirement already satisfied: databricks-vectorsearch==0.57 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (0.57)\n",
      "Requirement already satisfied: langchain==0.3.27 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-mcp in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (0.2.1)\n",
      "Requirement already satisfied: langchain_core==0.3.74 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (0.3.74)\n",
      "Requirement already satisfied: databricks-langchain==0.7.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (0.7.1)\n",
      "Requirement already satisfied: bs4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (0.0.2)\n",
      "Requirement already satisfied: dotenv in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (0.9.9)\n",
      "Requirement already satisfied: psycopg2-binary==2.9.9 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (2.9.9)\n",
      "Requirement already satisfied: pgvector==0.2.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (0.2.5)\n",
      "Requirement already satisfied: langgraph==0.3.4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (0.3.4)\n",
      "Requirement already satisfied: requests<3,>=2.28.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from databricks-sdk==0.61.0) (2.32.5)\n",
      "Requirement already satisfied: google-auth~=2.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-sdk==0.61.0) (2.38.0)\n",
      "Requirement already satisfied: googleapis-common-protos>=1.56.4 in /databricks/python3/lib/python3.12/site-packages (from databricks-connect==16.4.2) (1.69.2)\n",
      "Requirement already satisfied: grpcio-status>=1.59.3 in /databricks/python3/lib/python3.12/site-packages (from databricks-connect==16.4.2) (1.71.0)\n",
      "Requirement already satisfied: grpcio>=1.59.3 in /databricks/python3/lib/python3.12/site-packages (from databricks-connect==16.4.2) (1.71.0)\n",
      "Requirement already satisfied: numpy<2,>=1.15 in /databricks/python3/lib/python3.12/site-packages (from databricks-connect==16.4.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=23.2 in /databricks/python3/lib/python3.12/site-packages (from databricks-connect==16.4.2) (24.1)\n",
      "Requirement already satisfied: pandas>=1.0.5 in /databricks/python3/lib/python3.12/site-packages (from databricks-connect==16.4.2) (1.5.3)\n",
      "Requirement already satisfied: py4j==0.10.9.9 in /databricks/python3/lib/python3.12/site-packages (from databricks-connect==16.4.2) (0.10.9.9)\n",
      "Requirement already satisfied: setuptools>=68.0.0 in /usr/local/lib/python3.12/dist-packages (from databricks-connect==16.4.2) (75.8.0)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from databricks-connect==16.4.2) (1.16.0)\n",
      "Requirement already satisfied: dataclasses-json in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from databricks-agents==1.4.0) (0.6.7)\n",
      "Requirement already satisfied: jinja2>=3.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from databricks-agents==1.4.0) (3.1.6)\n",
      "Requirement already satisfied: mlflow-skinny<4.0.0,>=3.1.4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from databricks-agents==1.4.0) (3.2.0)\n",
      "Requirement already satisfied: tenacity>=8.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from databricks-agents==1.4.0) (9.1.2)\n",
      "Requirement already satisfied: tiktoken>=0.8.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from databricks-agents==1.4.0) (0.11.0)\n",
      "Requirement already satisfied: tqdm in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from databricks-agents==1.4.0) (4.67.1)\n",
      "Requirement already satisfied: urllib3>=2.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-agents==1.4.0) (2.2.2)\n",
      "Requirement already satisfied: pydantic>=2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from databricks-agents==1.4.0) (2.11.9)\n",
      "Requirement already satisfied: whenever==0.7.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from databricks-agents==1.4.0) (0.7.3)\n",
      "Requirement already satisfied: boto3>1 in /databricks/python3/lib/python3.12/site-packages (from databricks-agents==1.4.0) (1.34.69)\n",
      "Requirement already satisfied: botocore in /databricks/python3/lib/python3.12/site-packages (from databricks-agents==1.4.0) (1.34.69)\n",
      "Requirement already satisfied: protobuf<6,>=3.12.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-vectorsearch==0.57) (5.29.4)\n",
      "Requirement already satisfied: deprecation>=2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from databricks-vectorsearch==0.57) (2.1.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from langchain==0.3.27) (0.3.9)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from langchain==0.3.27) (0.4.28)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from langchain==0.3.27) (2.0.43)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /databricks/python3/lib/python3.12/site-packages (from langchain==0.3.27) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from langchain_core==0.3.74) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from langchain_core==0.3.74) (4.15.0)\n",
      "Requirement already satisfied: databricks-ai-bridge>=0.7.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from databricks-langchain==0.7.1) (0.8.0)\n",
      "Requirement already satisfied: openai>=1.99.9 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from databricks-langchain==0.7.1) (1.108.0)\n",
      "Requirement already satisfied: unitycatalog-langchain>=0.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.1) (0.2.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from langgraph==0.3.4) (2.1.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<0.2,>=0.1.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from langgraph==0.3.4) (0.1.8)\n",
      "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from langgraph==0.3.4) (0.1.74)\n",
      "Requirement already satisfied: ipython<10,>=8 in /databricks/python3/lib/python3.12/site-packages (from databricks-sdk[notebook]) (8.32.0)\n",
      "Requirement already satisfied: ipywidgets<9,>=8 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from databricks-sdk[notebook]) (8.1.7)\n",
      "Requirement already satisfied: mlflow-tracing==3.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from mlflow<=3.2) (3.2.0)\n",
      "Requirement already satisfied: Flask<4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from mlflow<=3.2) (3.1.2)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from mlflow<=3.2) (1.16.5)\n",
      "Requirement already satisfied: docker<8,>=4.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from mlflow<=3.2) (7.1.0)\n",
      "Requirement already satisfied: graphene<4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from mlflow<=3.2) (3.4.3)\n",
      "Requirement already satisfied: gunicorn<24 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from mlflow<=3.2) (23.0.0)\n",
      "Requirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.12/site-packages (from mlflow<=3.2) (3.8.4)\n",
      "Requirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.12/site-packages (from mlflow<=3.2) (1.4.2)\n",
      "Requirement already satisfied: scipy<2 in /databricks/python3/lib/python3.12/site-packages (from mlflow<=3.2) (1.13.1)\n",
      "Requirement already satisfied: cachetools<7,>=5.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4.0.0,>=3.1.4->databricks-agents==1.4.0) (5.3.3)\n",
      "Requirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4.0.0,>=3.1.4->databricks-agents==1.4.0) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle<4 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4.0.0,>=3.1.4->databricks-agents==1.4.0) (3.0.0)\n",
      "Requirement already satisfied: fastapi<1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4.0.0,>=3.1.4->databricks-agents==1.4.0) (0.115.12)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4.0.0,>=3.1.4->databricks-agents==1.4.0) (3.1.37)\n",
      "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4.0.0,>=3.1.4->databricks-agents==1.4.0) (7.0.1)\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4.0.0,>=3.1.4->databricks-agents==1.4.0) (1.31.1)\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4.0.0,>=3.1.4->databricks-agents==1.4.0) (1.31.1)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4.0.0,>=3.1.4->databricks-agents==1.4.0) (0.5.3)\n",
      "Requirement already satisfied: uvicorn<1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4.0.0,>=3.1.4->databricks-agents==1.4.0) (0.34.0)\n",
      "Requirement already satisfied: azure-storage-file-datalake>12 in /databricks/python3/lib/python3.12/site-packages (from mlflow[databricks]) (12.17.0)\n",
      "Requirement already satisfied: google-cloud-storage>=1.30.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow[databricks]) (3.1.0)\n",
      "Requirement already satisfied: mcp~=1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from langchain-mcp) (1.14.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from bs4) (4.13.5)\n",
      "Requirement already satisfied: python-dotenv in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from dotenv) (1.1.1)\n",
      "Requirement already satisfied: Mako in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from alembic!=1.10.0,<2->mlflow<=3.2) (1.3.10)\n",
      "Requirement already satisfied: azure-core>=1.30.0 in /databricks/python3/lib/python3.12/site-packages (from azure-storage-file-datalake>12->mlflow[databricks]) (1.33.0)\n",
      "Requirement already satisfied: azure-storage-blob>=12.23.0 in /databricks/python3/lib/python3.12/site-packages (from azure-storage-file-datalake>12->mlflow[databricks]) (12.23.0)\n",
      "Requirement already satisfied: isodate>=0.6.1 in /databricks/python3/lib/python3.12/site-packages (from azure-storage-file-datalake>12->mlflow[databricks]) (0.7.2)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /databricks/python3/lib/python3.12/site-packages (from boto3>1->databricks-agents==1.4.0) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /databricks/python3/lib/python3.12/site-packages (from boto3>1->databricks-agents==1.4.0) (0.10.4)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /databricks/python3/lib/python3.12/site-packages (from botocore->databricks-agents==1.4.0) (2.9.0.post0)\n",
      "Requirement already satisfied: tabulate>=0.9.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from databricks-ai-bridge>=0.7.1->databricks-langchain==0.7.1) (0.9.0)\n",
      "Requirement already satisfied: langchain-openai in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from databricks-sdk[openai]>=0.58.0->databricks-agents==1.4.0) (0.3.32)\n",
      "Requirement already satisfied: httpx in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from databricks-sdk[openai]>=0.58.0->databricks-agents==1.4.0) (0.28.1)\n",
      "Requirement already satisfied: blinker>=1.9.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from Flask<4->mlflow<=3.2) (1.9.0)\n",
      "Requirement already satisfied: itsdangerous>=2.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from Flask<4->mlflow<=3.2) (2.2.0)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from Flask<4->mlflow<=3.2) (3.0.2)\n",
      "Requirement already satisfied: werkzeug>=3.1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from Flask<4->mlflow<=3.2) (3.1.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk==0.61.0) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk==0.61.0) (4.9)\n",
      "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /databricks/python3/lib/python3.12/site-packages (from google-cloud-storage>=1.30.0->mlflow[databricks]) (2.20.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.4.2 in /databricks/python3/lib/python3.12/site-packages (from google-cloud-storage>=1.30.0->mlflow[databricks]) (2.4.3)\n",
      "Requirement already satisfied: google-resumable-media>=2.7.2 in /databricks/python3/lib/python3.12/site-packages (from google-cloud-storage>=1.30.0->mlflow[databricks]) (2.7.2)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /databricks/python3/lib/python3.12/site-packages (from google-cloud-storage>=1.30.0->mlflow[databricks]) (1.7.1)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from graphene<4->mlflow<=3.2) (3.2.6)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from graphene<4->mlflow<=3.2) (3.2.0)\n",
      "Requirement already satisfied: decorator in /databricks/python3/lib/python3.12/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /databricks/python3/lib/python3.12/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /databricks/python3/lib/python3.12/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (0.1.6)\n",
      "Requirement already satisfied: pexpect>4.3 in /databricks/python3/lib/python3.12/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (4.8.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /databricks/python3/lib/python3.12/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /databricks/python3/lib/python3.12/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (2.15.1)\n",
      "Requirement already satisfied: stack_data in /databricks/python3/lib/python3.12/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /databricks/python3/lib/python3.12/site-packages (from ipython<10,>=8->databricks-sdk[notebook]) (5.14.3)\n",
      "Requirement already satisfied: comm>=0.1.3 in /databricks/python3/lib/python3.12/site-packages (from ipywidgets<9,>=8->databricks-sdk[notebook]) (0.2.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from ipywidgets<9,>=8->databricks-sdk[notebook]) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from ipywidgets<9,>=8->databricks-sdk[notebook]) (3.0.15)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain_core==0.3.74) (3.0.0)\n",
      "Requirement already satisfied: ormsgpack>=1.10.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph==0.3.4) (1.10.0)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.3.4) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain==0.3.27) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /databricks/python3/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain==0.3.27) (0.23.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow<=3.2) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow<=3.2) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow<=3.2) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow<=3.2) (1.4.4)\n",
      "Requirement already satisfied: pillow>=8 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow<=3.2) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow<=3.2) (3.0.9)\n",
      "Requirement already satisfied: anyio>=4.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from mcp~=1.0->langchain-mcp) (4.10.0)\n",
      "Requirement already satisfied: httpx-sse>=0.4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from mcp~=1.0->langchain-mcp) (0.4.1)\n",
      "Requirement already satisfied: jsonschema>=4.20.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from mcp~=1.0->langchain-mcp) (4.25.1)\n",
      "Requirement already satisfied: pydantic-settings>=2.5.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from mcp~=1.0->langchain-mcp) (2.10.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from mcp~=1.0->langchain-mcp) (0.0.20)\n",
      "Requirement already satisfied: sse-starlette>=1.6.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from mcp~=1.0->langchain-mcp) (3.0.2)\n",
      "Requirement already satisfied: starlette>=0.27 in /databricks/python3/lib/python3.12/site-packages (from mcp~=1.0->langchain-mcp) (0.46.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.99.9->databricks-langchain==0.7.1) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from openai>=1.99.9->databricks-langchain==0.7.1) (0.11.0)\n",
      "Requirement already satisfied: sniffio in /databricks/python3/lib/python3.12/site-packages (from openai>=1.99.9->databricks-langchain==0.7.1) (1.3.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.12/site-packages (from pandas>=1.0.5->databricks-connect==16.4.2) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /databricks/python3/lib/python3.12/site-packages (from pydantic>=2->databricks-agents==1.4.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from pydantic>=2->databricks-agents==1.4.0) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from pydantic>=2->databricks-agents==1.4.0) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2.28.1->databricks-sdk==0.61.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2.28.1->databricks-sdk==0.61.0) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2.28.1->databricks-sdk==0.61.0) (2024.6.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn<2->mlflow<=3.2) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn<2->mlflow<=3.2) (2.2.0)\n",
      "Requirement already satisfied: greenlet>=1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.27) (3.2.4)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from tiktoken>=0.8.0->databricks-agents==1.4.0) (2025.9.1)\n",
      "Requirement already satisfied: langchain-community>=0.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.1) (0.3.28)\n",
      "Requirement already satisfied: unitycatalog-ai in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.1) (0.3.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from beautifulsoup4->bs4) (2.8)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from dataclasses-json->databricks-agents==1.4.0) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from dataclasses-json->databricks-agents==1.4.0) (0.9.0)\n",
      "Requirement already satisfied: cryptography>=2.1.4 in /databricks/python3/lib/python3.12/site-packages (from azure-storage-blob>=12.23.0->azure-storage-file-datalake>12->mlflow[databricks]) (42.0.5)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny<4.0.0,>=3.1.4->databricks-agents==1.4.0) (4.0.11)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /databricks/python3/lib/python3.12/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage>=1.30.0->mlflow[databricks]) (1.26.1)\n",
      "Requirement already satisfied: httpcore==1.* in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from httpx->databricks-sdk[openai]>=0.58.0->databricks-agents==1.4.0) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from httpcore==1.*->httpx->databricks-sdk[openai]>=0.58.0->databricks-agents==1.4.0) (0.16.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.12/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny<4.0.0,>=3.1.4->databricks-agents==1.4.0) (3.17.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /databricks/python3/lib/python3.12/site-packages (from jedi>=0.16->ipython<10,>=8->databricks-sdk[notebook]) (0.8.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp~=1.0->langchain-mcp) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp~=1.0->langchain-mcp) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp~=1.0->langchain-mcp) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp~=1.0->langchain-mcp) (0.27.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.1) (3.12.15)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny<4.0.0,>=3.1.4->databricks-agents==1.4.0) (1.2.18)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.52b1 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny<4.0.0,>=3.1.4->databricks-agents==1.4.0) (0.52b1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /databricks/python3/lib/python3.12/site-packages (from pexpect>4.3->ipython<10,>=8->databricks-sdk[notebook]) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /databricks/python3/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython<10,>=8->databricks-sdk[notebook]) (0.2.5)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk==0.61.0) (0.4.8)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /databricks/python3/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->databricks-agents==1.4.0) (1.0.0)\n",
      "Requirement already satisfied: executing in /databricks/python3/lib/python3.12/site-packages (from stack_data->ipython<10,>=8->databricks-sdk[notebook]) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /databricks/python3/lib/python3.12/site-packages (from stack_data->ipython<10,>=8->databricks-sdk[notebook]) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /databricks/python3/lib/python3.12/site-packages (from stack_data->ipython<10,>=8->databricks-sdk[notebook]) (0.2.2)\n",
      "Requirement already satisfied: nest-asyncio in /databricks/python3/lib/python3.12/site-packages (from unitycatalog-ai->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.1) (1.6.0)\n",
      "Requirement already satisfied: unitycatalog-client in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from unitycatalog-ai->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.1) (0.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.1) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.1) (1.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.1) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.1) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.1) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.0->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.1) (1.20.1)\n",
      "Requirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.12/site-packages (from cryptography>=2.1.4->azure-storage-blob>=12.23.0->azure-storage-file-datalake>12->mlflow[databricks]) (1.16.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /databricks/python3/lib/python3.12/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny<4.0.0,>=3.1.4->databricks-agents==1.4.0) (1.14.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny<4.0.0,>=3.1.4->databricks-agents==1.4.0) (5.0.0)\n",
      "Requirement already satisfied: aiohttp-retry>=2.8.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-ccb7b741-22d3-462d-bc3f-f62428cc0d4a/lib/python3.12/site-packages (from unitycatalog-client->unitycatalog-ai->unitycatalog-langchain>=0.2.0->unitycatalog-langchain[databricks]>=0.2.0->databricks-langchain==0.7.1) (2.9.1)\n",
      "Requirement already satisfied: pycparser in /databricks/python3/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob>=12.23.0->azure-storage-file-datalake>12->mlflow[databricks]) (2.21)\n",
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install 'databricks-sdk==0.61.0' 'databricks-connect==16.4.2' 'pyarrow<20' 'databricks-sdk[notebook]' 'databricks-agents==1.4.0' 'mlflow<=3.2' 'mlflow[databricks]' 'databricks-vectorsearch==0.57' 'langchain==0.3.27' 'langchain-mcp' 'langchain_core==0.3.74' 'databricks-langchain==0.7.1' 'bs4' 'dotenv' 'psycopg2-binary==2.9.9' 'pgvector==0.2.5' 'langgraph==0.3.4'\n",
    "import os\n",
    "if os.environ.get(\"DATABRICKS_RUNTIME_VERSION\"):\n",
    "    dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7aed6b4a-e10c-4d2f-9359-bddb7e481a92",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Write Langgraph Agent"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting chain_postgres_genie.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile chain_postgres_genie.py\n",
    "import functools\n",
    "import os\n",
    "import uuid\n",
    "import json\n",
    "from typing import Any, Generator, Literal, Optional, Dict, List\n",
    "\n",
    "import mlflow\n",
    "import pydantic\n",
    "from mlflow.models import ModelConfig\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks_langchain import (\n",
    "    ChatDatabricks,\n",
    "    UCFunctionToolkit,\n",
    "    DatabricksFunctionClient,\n",
    "    set_uc_function_client\n",
    ")\n",
    "from databricks_langchain.genie import GenieAgent\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from mlflow.langchain.chat_agent_langgraph import ChatAgentState\n",
    "from mlflow.pyfunc import ChatAgent\n",
    "from mlflow.types.agent import (\n",
    "    ChatAgentChunk,\n",
    "    ChatAgentMessage,\n",
    "    ChatAgentResponse,\n",
    "    ChatContext,\n",
    ")\n",
    "from pydantic import BaseModel\n",
    "from sqlalchemy import create_engine, text, event\n",
    "from pgvector.psycopg2 import register_vector\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks_langchain import DatabricksEmbeddings\n",
    "from databricks_langchain.chat_models import ChatDatabricks\n",
    "from langchain.tools import Tool\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from mlflow.entities import SpanType, Document\n",
    "from langchain_core.messages.ai import AIMessage\n",
    "\n",
    "# Enable MLflow Tracing for LangChain\n",
    "mlflow.autolog()\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "# Load chain configuration provided at logging/deployment time.\n",
    "model_config: ModelConfig = mlflow.models.ModelConfig()\n",
    "\n",
    "# Pydantic models for input validation\n",
    "class Message(pydantic.BaseModel):\n",
    "    role: str\n",
    "    content: str\n",
    "    name: Optional[str] = None\n",
    "\n",
    "class Filters(pydantic.BaseModel):\n",
    "    user_name: str  # Required\n",
    "    chat_id: Optional[str] = None\n",
    "\n",
    "class CustomInputs(pydantic.BaseModel):\n",
    "    filters: Filters\n",
    "    k: Optional[int] = None  # Optional, will default to model_config value\n",
    "\n",
    "class ChatRequest(pydantic.BaseModel):\n",
    "    messages: List[Message]\n",
    "    custom_inputs: Optional[CustomInputs] = None\n",
    "\n",
    "class ChatResponse(pydantic.BaseModel):\n",
    "    messages: List[Message]\n",
    "    finish_reason: Optional[str] = None\n",
    "\n",
    "\n",
    "def _get_required_env(name: str) -> str:\n",
    "    value = os.environ.get(name)\n",
    "    if not value:\n",
    "        raise RuntimeError(f\"Missing required environment variable: {name}\")\n",
    "    return value\n",
    "\n",
    "\n",
    "def get_postgres_connection(\n",
    "    client: WorkspaceClient,\n",
    "    db_instance_name: str,\n",
    "    database_name: Optional[str] = \"databricks_postgres\",\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Build a PostgreSQL SQLAlchemy URL (psycopg2) using Databricks Database credentials.\n",
    "\n",
    "    Uses POSTGRES_GROUP env var as username if set; otherwise current user.\n",
    "    Always enforces sslmode=require.\n",
    "    \"\"\"\n",
    "    database = client.database.get_database_instance(db_instance_name)\n",
    "    credentials = client.database.generate_database_credential(\n",
    "        instance_names=[db_instance_name], request_id=str(uuid.uuid4())\n",
    "    )\n",
    "\n",
    "    postgres_group = os.getenv(\"POSTGRES_GROUP\")\n",
    "    username = (\n",
    "        postgres_group if postgres_group else client.current_user.me().user_name\n",
    "    )\n",
    "\n",
    "    host = database.read_write_dns\n",
    "    port = \"5432\"\n",
    "    password = credentials.token\n",
    "    db_name = database_name or \"databricks_postgres\"\n",
    "\n",
    "    # SQLAlchemy URL with psycopg2 driver\n",
    "    sqlalchemy_url = (\n",
    "        f\"postgresql+psycopg2://{username}:{password}@{host}:{port}/{db_name}?sslmode=require\"\n",
    "    )\n",
    "    return sqlalchemy_url\n",
    "\n",
    "\n",
    "# --- Databricks Auth (required for both embeddings and DB credentials) ---\n",
    "_DATABRICKS_HOST = _get_required_env(\"DATABRICKS_HOST\")\n",
    "_DATABRICKS_TOKEN = _get_required_env(\"DATABRICKS_TOKEN\")\n",
    "\n",
    "workspace_client = WorkspaceClient(host=_DATABRICKS_HOST, token=_DATABRICKS_TOKEN)\n",
    "\n",
    "\n",
    "# --- Postgres Engine (pgvector) ---\n",
    "def _build_engine() -> Any:\n",
    "    # Allow configuration via model_config or environment variables\n",
    "    db_instance_name = (\n",
    "        os.environ.get(\"DATABASE_INSTANCE_NAME\")\n",
    "        or model_config.get(\"database_instance_name\")\n",
    "    )\n",
    "    if not db_instance_name:\n",
    "        raise RuntimeError(\n",
    "            \"A Postgres database instance name is required. Set env 'DATABASE_INSTANCE_NAME' \"\n",
    "            \"or include 'database_instance_name' in the model_config.\"\n",
    "        )\n",
    "\n",
    "    postgres_database_name = (\n",
    "        os.environ.get(\"POSTGRES_DATABASE_NAME\")\n",
    "        or model_config.get(\"postgres_database_name\")\n",
    "        or \"databricks_postgres\"\n",
    "    )\n",
    "\n",
    "    database_url = get_postgres_connection(\n",
    "        workspace_client, db_instance_name, postgres_database_name\n",
    "    )\n",
    "\n",
    "    engine = create_engine(database_url, pool_pre_ping=True)\n",
    "\n",
    "    @event.listens_for(engine, \"connect\")\n",
    "    def _register_vector(dbapi_connection, connection_record):  # noqa: ANN001\n",
    "        # Map Python lists to pgvector type for psycopg2\n",
    "        register_vector(dbapi_connection)\n",
    "\n",
    "    return engine\n",
    "\n",
    "\n",
    "engine = _build_engine()\n",
    "\n",
    "\n",
    "# --- Embeddings ---\n",
    "embeddings = DatabricksEmbeddings(\n",
    "    endpoint=model_config.get(\"embedding_model\"),\n",
    "    token=_DATABRICKS_TOKEN,\n",
    ")\n",
    "\n",
    "# --- Vector similarity search over Postgres (pgvector) ---\n",
    "@mlflow.trace\n",
    "def pg_vector_similarity_search(\n",
    "    query_text: str,\n",
    "    k: int = 3,\n",
    "    filters: Optional[Dict[str, Any]] = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Perform similarity search against message embeddings in Postgres (pgvector).\n",
    "\n",
    "    Schema expectations:\n",
    "    - message_embeddings(me: id, message_id, user_name, chat_id, embedding vector)\n",
    "    - chat_history(ch: id, message_content, message_type, created_at, message_order)\n",
    "    \"\"\"\n",
    "    filters = filters or {}\n",
    "\n",
    "    # 1) Embed the query\n",
    "    query_embedding = embeddings.embed_query(query_text)\n",
    "\n",
    "    # 2) WHERE clause from filters\n",
    "    where_conditions: List[str] = []\n",
    "    params: Dict[str, Any] = {}\n",
    "\n",
    "    if \"user_name\" in filters:\n",
    "        where_conditions.append(\"me.user_name = :user_name\")\n",
    "        params[\"user_name\"] = filters[\"user_name\"]\n",
    "\n",
    "    where_clause = \"\"\n",
    "    if where_conditions:\n",
    "        where_clause = \"WHERE \" + \" AND \".join(where_conditions)\n",
    "\n",
    "    # 3) Query using cosine distance operator (<=>) provided by pgvector\n",
    "    sql = text(\n",
    "        f\"\"\"\n",
    "        SELECT\n",
    "            ch.message_content,\n",
    "            me.user_name,\n",
    "            me.chat_id,\n",
    "            ch.message_type,\n",
    "            ch.created_at,\n",
    "            ch.message_order,\n",
    "            (me.embedding <=> CAST(:query_embedding AS vector)) AS distance\n",
    "        FROM message_embeddings me\n",
    "        JOIN chat_history ch ON me.message_id = ch.id\n",
    "        {where_clause}\n",
    "        ORDER BY me.embedding <=> CAST(:query_embedding AS vector)\n",
    "        LIMIT :k\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    span = mlflow.get_current_active_span()\n",
    "    span.set_outputs([Document(page_content=sql)])\n",
    "\n",
    "    with engine.connect() as conn:\n",
    "        rows = conn.execute(\n",
    "            sql, {\"query_embedding\": query_embedding, \"k\": k, **params}\n",
    "        ).fetchall()\n",
    "\n",
    "    passages = [f\"Passage: {r.message_content}\" for r in rows]\n",
    "    return \"\\n\".join(passages)\n",
    "  \n",
    "\n",
    "def create_context_aware_vector_search_tool(state, custom_k: Optional[int] = None):\n",
    "    \"\"\"Create a vector search tool that has access to user context from state\"\"\"\n",
    "    \n",
    "    def filtered_vector_search(query: str) -> str:\n",
    "        # Extract user context from state\n",
    "        user_context = state.get(\"user_context\", {})\n",
    "        filters = user_context.get(\"filters\", {})\n",
    "        \n",
    "        # Use custom k if provided, otherwise fall back to model_config default\n",
    "        k = custom_k if custom_k is not None else model_config.get('k')\n",
    "        \n",
    "        # Use your existing pg_vector_similarity_search with filters and custom k\n",
    "        return pg_vector_similarity_search(\n",
    "            query_text=query, \n",
    "            k=k, \n",
    "            filters=filters\n",
    "        )\n",
    "    \n",
    "    return Tool(\n",
    "        name=\"search_chat_history\",\n",
    "        description=\"Retrieve chat history from Postgres (pgvector) for the current user; use only if the immediate conversation context is insufficient. The input to this function should be the user message.\",\n",
    "        func=filtered_vector_search,\n",
    "    )\n",
    "\n",
    "\n",
    "# Marketing Policy Agent - Knowledge Assistant Integration\n",
    "class MarketingPolicyAgent:\n",
    "    \"\"\"Agent for validating marketing policy compliance using Databricks Knowledge Assistant\"\"\"\n",
    "    \n",
    "    def __init__(self, endpoint_name: str, client: WorkspaceClient, description: str):\n",
    "        self.endpoint_name = endpoint_name\n",
    "        self.client = client\n",
    "        self.description = description\n",
    "        \n",
    "    def invoke(self, state):\n",
    "        \"\"\"Invoke the marketing policy agent via Databricks serving endpoint\"\"\"\n",
    "        try:\n",
    "            messages = state.get(\"messages\", [])\n",
    "            \n",
    "            # Build the request for the knowledge assistant\n",
    "            payload = { 'input': messages}\n",
    "            \n",
    "            # Call the knowledge assistant endpoint\n",
    "            response = self.client.api_client.do(\n",
    "                method=\"POST\",\n",
    "                path=f\"/serving-endpoints/{self.endpoint_name}/invocations\",\n",
    "                headers={\"Content-Type\": \"application/json\"},\n",
    "                data=json.dumps(payload)\n",
    "            )\n",
    "            \n",
    "            # Extract content from response\n",
    "            content = \"\"\n",
    "            if isinstance(response, dict):\n",
    "                content = response['output'][0][\"content\"][0]['text']\n",
    "            else:\n",
    "                content = str(response)\n",
    "            \n",
    "            # Return in the expected format\n",
    "            return {\n",
    "                \"messages\": [AIMessage(content=content)]\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"messages\": [{\n",
    "                    \"role\": \"assistant\", \n",
    "                    \"content\": f\"Error validating marketing policy compliance: {str(e)}\"\n",
    "                }]\n",
    "            }\n",
    "\n",
    "\n",
    "genie_agent_description = model_config.get('genie_agent_description')\n",
    "general_assistant_description = model_config.get('general_assistant_description')\n",
    "marketing_policy_agent_description = model_config.get('marketing_policy_agent_description')\n",
    "\n",
    "genie_agent = GenieAgent(\n",
    "    genie_space_id=model_config.get('genie_space_id'),\n",
    "    genie_agent_name=\"Genie\",\n",
    "    description=genie_agent_description,\n",
    "    client=workspace_client,\n",
    "    include_context=True,\n",
    ")\n",
    "\n",
    "# Create Marketing Policy Agent\n",
    "marketing_policy_agent = MarketingPolicyAgent(\n",
    "    endpoint_name=model_config.get('marketing_policy_endpoint'),\n",
    "    client=workspace_client,\n",
    "    description=marketing_policy_agent_description\n",
    ")\n",
    "\n",
    "# Max number of interactions between agents\n",
    "MAX_ITERATIONS = 3\n",
    "\n",
    "worker_descriptions = {\n",
    "    \"Genie\": genie_agent_description,\n",
    "    \"General\": general_assistant_description,\n",
    "    \"MarketingPolicy\": marketing_policy_agent_description,\n",
    "}\n",
    "\n",
    "formatted_descriptions = \"\\n\".join(\n",
    "    f\"- {name}: {desc}\" for name, desc in worker_descriptions.items()\n",
    ")\n",
    "\n",
    "system_prompt = f\"\"\"You are routing between specialized agents. Route to:\n",
    "- Genie: For data queries requiring database access.\n",
    "- General: To synthesize and present final answers when sufficient data is available.\n",
    "- MarketingPolicy: To validate marketing compliance, policy adherence, and brand guidelines.\n",
    "- FINISH: When a complete answer has been provided\n",
    "\n",
    "Available agents:\n",
    "{formatted_descriptions}\"\"\"\n",
    "\n",
    "options = [\"FINISH\"] + list(worker_descriptions.keys())\n",
    "FINISH = {\"next_node\": \"FINISH\"}\n",
    "MARKETING_POLICY = {\"next_node\": \"MarketingPolicy\"}\n",
    "\n",
    "# Our foundation model answering the final prompt\n",
    "model = ChatDatabricks(\n",
    "    endpoint=model_config.get(\"llm_model_serving_endpoint_name\"),\n",
    "    extra_params={\"temperature\": 0.01, \"max_tokens\": 500}\n",
    ")\n",
    "\n",
    "# Custom Static Tools\n",
    "tools = []\n",
    "\n",
    "def supervisor_agent(state):\n",
    "    count = state.get(\"iteration_count\", 0) + 1\n",
    "    if count > MAX_ITERATIONS:\n",
    "        return FINISH\n",
    "    \n",
    "    # Check if Genie just provided a data-rich response\n",
    "    messages = state.get(\"messages\", [])\n",
    "    if messages:\n",
    "        last_message = messages[-1] if messages else {}\n",
    "        \n",
    "        if (isinstance(last_message, dict) and \n",
    "            last_message.get(\"name\") == \"Genie\" and \n",
    "            last_message.get(\"content\", \"\").strip() and\n",
    "            len(last_message.get(\"content\", \"\")) > 50):  # Assume substantial data\n",
    "            return FINISH\n",
    "    \n",
    "    class nextNode(BaseModel):\n",
    "        next_node: Literal[tuple(options)]\n",
    "\n",
    "    preprocessor = RunnableLambda(\n",
    "        lambda state: [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "    )\n",
    "    supervisor_chain = preprocessor | model.with_structured_output(nextNode)\n",
    "    next_node = supervisor_chain.invoke(state).next_node\n",
    "    \n",
    "    # if routed back to the same node, exit the loop\n",
    "    if state.get(\"next_node\") == next_node:\n",
    "        return FINISH\n",
    "    return {\n",
    "        \"iteration_count\": count,\n",
    "        \"next_node\": next_node\n",
    "    }\n",
    "\n",
    "#######################################\n",
    "# Define our multiagent graph structure\n",
    "#######################################\n",
    "\n",
    "\n",
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": result[\"messages\"][-1].content,\n",
    "                \"name\": name,\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "def final_answer(state):\n",
    "    # Check if we have data-rich responses from Genie\n",
    "    messages = state.get(\"messages\", [])\n",
    "    prompt = \"Using only the content in the messages, respond to the previous user question using the answer given by the other assistant messages.\"\n",
    "    \n",
    "    preprocessor = RunnableLambda(\n",
    "        lambda state: state[\"messages\"] + [{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    final_answer_chain = preprocessor | model\n",
    "    return {\"messages\": [final_answer_chain.invoke(state)]}\n",
    "\n",
    "\n",
    "def agent_node_with_context(state, agent, name, custom_k: Optional[int] = None):\n",
    "    \"\"\"Enhanced agent node that injects context-aware tools\"\"\"\n",
    "    \n",
    "    # Create the shared vector search tool with current state context and custom k\n",
    "    vector_search_tool = create_context_aware_vector_search_tool(state, custom_k)\n",
    "    \n",
    "    if name == \"Genie\":\n",
    "        # Genie already has its tools, just add vector search\n",
    "        enhanced_agent = agent  # Genie agent already configured\n",
    "        \n",
    "    elif name == \"MarketingPolicy\":\n",
    "        # Marketing Policy agent already configured\n",
    "        enhanced_agent = agent\n",
    "        \n",
    "    elif name == \"General\":\n",
    "        # Add vector search tool to General agent\n",
    "        enhanced_tools = tools + [vector_search_tool]\n",
    "        enhanced_agent = create_react_agent(model, tools=enhanced_tools)\n",
    "        \n",
    "    # Execute with enhanced agent\n",
    "    result = enhanced_agent.invoke(state)\n",
    "    return {\n",
    "        \"messages\": [{\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": result[\"messages\"][-1].content,\n",
    "            \"name\": name,\n",
    "        }]\n",
    "    }\n",
    "\n",
    "class AgentState(ChatAgentState):\n",
    "    next_node: str\n",
    "    iteration_count: int\n",
    "    user_context: Optional[Dict[str, Any]] = None\n",
    "    custom_k: Optional[int] = None\n",
    "\n",
    "# Create enhanced agent nodes\n",
    "def enhanced_genie_node(state):\n",
    "    custom_k = state.get(\"custom_k\")\n",
    "    return agent_node_with_context(state, genie_agent, \"Genie\", custom_k)\n",
    "\n",
    "def enhanced_general_node(state):\n",
    "    custom_k = state.get(\"custom_k\")\n",
    "    return agent_node_with_context(state, None, \"General\", custom_k)\n",
    "\n",
    "def enhanced_marketing_policy_node(state):\n",
    "    custom_k = state.get(\"custom_k\")\n",
    "    return agent_node_with_context(state, marketing_policy_agent, \"MarketingPolicy\", custom_k)\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "# Agent States\n",
    "workflow.add_node(\"Genie\", enhanced_genie_node)\n",
    "workflow.add_node(\"General\", enhanced_general_node)\n",
    "workflow.add_node(\"MarketingPolicy\", enhanced_marketing_policy_node)\n",
    "# Supervisor States\n",
    "workflow.add_node(\"supervisor\", supervisor_agent)\n",
    "workflow.add_node(\"final_answer\", final_answer)\n",
    "\n",
    "workflow.set_entry_point(\"supervisor\")\n",
    "# We want our workers to ALWAYS \"report back\" to the supervisor when done\n",
    "for worker in worker_descriptions.keys():\n",
    "    workflow.add_edge(worker, \"supervisor\")\n",
    "\n",
    "# Let the supervisor decide which next node to go\n",
    "workflow.add_conditional_edges(\n",
    "    \"supervisor\",\n",
    "    lambda x: x[\"next_node\"],\n",
    "    {**{k: k for k in worker_descriptions.keys()}, \"FINISH\": \"final_answer\"},\n",
    ")\n",
    "workflow.add_edge(\"final_answer\", END)\n",
    "multi_agent = workflow.compile()\n",
    "\n",
    "###################################\n",
    "# Streaming LangGraph ChatAgent\n",
    "###################################\n",
    "\n",
    "class PostgresGenieChatAgent(ChatAgent):\n",
    "    def __init__(self, agent: CompiledStateGraph):\n",
    "        self.agent = agent\n",
    "    \n",
    "    def predict(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> ChatAgentResponse:\n",
    "        \"\"\"Non-streaming predict method for backward compatibility\"\"\"\n",
    "        # Extract user context and custom_k from custom_inputs\n",
    "        user_context = {}\n",
    "        custom_k = None\n",
    "        \n",
    "        if custom_inputs:\n",
    "            if \"filters\" in custom_inputs:\n",
    "                user_context[\"filters\"] = custom_inputs[\"filters\"]\n",
    "            custom_k = custom_inputs.get(\"k\")\n",
    "        \n",
    "        agent_request = {\n",
    "            \"messages\": [m.model_dump_compat(exclude_none=True) for m in messages],\n",
    "            \"user_context\": user_context,\n",
    "            \"custom_k\": custom_k\n",
    "        }\n",
    "\n",
    "        response_messages = []\n",
    "        for event in self.agent.stream(agent_request, stream_mode=\"updates\"):\n",
    "            for node_data in event.values():\n",
    "                response_messages.extend(\n",
    "                    ChatAgentMessage(**msg) for msg in node_data.get(\"messages\", [])\n",
    "                )\n",
    "        \n",
    "        return ChatAgentResponse(messages=response_messages)\n",
    "\n",
    "    def predict_stream(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> Generator[ChatAgentChunk, None, None]:\n",
    "        \"\"\"Streaming predict method - yields incremental responses as they're generated\"\"\"\n",
    "        # Extract user context and custom_k from custom_inputs\n",
    "        user_context = {}\n",
    "        custom_k = None\n",
    "        \n",
    "        if custom_inputs:\n",
    "            if \"filters\" in custom_inputs:\n",
    "                user_context[\"filters\"] = custom_inputs[\"filters\"]\n",
    "            custom_k = custom_inputs.get(\"k\")\n",
    "        \n",
    "        agent_request = {\n",
    "            \"messages\": [m.model_dump_compat(exclude_none=True) for m in messages],\n",
    "            \"user_context\": user_context,\n",
    "            \"custom_k\": custom_k\n",
    "        }\n",
    "\n",
    "        for event in self.agent.stream(agent_request, stream_mode=\"updates\"):\n",
    "            for node_data in event.values():\n",
    "                yield from (\n",
    "                    ChatAgentChunk(**{\"delta\": msg})\n",
    "                    for msg in node_data.get(\"messages\", [])\n",
    "                )\n",
    "\n",
    "# Create the streaming model instance and set it for MLflow\n",
    "streaming_model_instance = PostgresGenieChatAgent(multi_agent)\n",
    "mlflow.models.set_model(model=streaming_model_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "354c76a9-8be0-4c8d-9983-9e57eb961e82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"embedding_model\", \"databricks-gte-large-en\")\n",
    "dbutils.widgets.text(\"database_instance_name\", \"adtech-series-do-not-delete\")\n",
    "dbutils.widgets.text(\"postgres_database_name\", \"databricks_postgres\")\n",
    "dbutils.widgets.text(\"llm_model_serving_endpoint_name\", \"databricks-claude-3-7-sonnet\")\n",
    "dbutils.widgets.text(\"target_catalog\", \"tanner_wendland\")\n",
    "dbutils.widgets.text(\"target_schema\", \"default\")\n",
    "dbutils.widgets.text(\"genie_space_id\", \"01f07c4ba44615aab8989b10e0a95420\")\n",
    "dbutils.widgets.text(\"marketing_policy_endpoint\", \"ka-c9af3fe4-endpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c99bd51e-5492-4d5d-a971-0ddda14452cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "embedding_model = dbutils.widgets.get(\"embedding_model\")\n",
    "database_instance_name = dbutils.widgets.get(\"database_instance_name\")\n",
    "postgres_database_name = dbutils.widgets.get(\"postgres_database_name\")\n",
    "llm_model_serving_endpoint_name = dbutils.widgets.get(\"llm_model_serving_endpoint_name\")\n",
    "target_catalog = dbutils.widgets.get(\"target_catalog\")\n",
    "target_schema = dbutils.widgets.get(\"target_schema\")\n",
    "genie_space_id = dbutils.widgets.get(\"genie_space_id\")\n",
    "marketing_policy_endpoint = dbutils.widgets.get(\"marketing_policy_endpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2750086a-7c5b-40aa-b5f6-28fec9147b79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "from typing import Any, Generator, Literal, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d115d39-47b3-40a6-b143-e4ae3f30c1c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/19 17:02:02 WARNING mlflow.tracking.fluent: Exception raised while enabling autologging for pyspark: MLflow Spark dataset autologging is not supported on Databricks shared clusters or Databricks serverless clusters.\n",
      "2025/09/19 17:02:02 WARNING mlflow.tracking.fluent: Exception raised while enabling autologging for pyspark.ml: [JVM_ATTRIBUTE_NOT_SUPPORTED] Attribute `sparkContext` is not supported in Spark Connect as it depends on the JVM. If you need to use this attribute, do not use Spark Connect when creating your session. Visit https://spark.apache.org/docs/latest/sql-getting-started.html#starting-point-sparksession for creating regular Spark Session in detail.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "mlflow.autolog()\n",
    "mlflow.langchain.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "585760d5-cc0a-4a30-b069-1b57188e8623",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Chain Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff9d15d1-397a-4cdf-b948-41a6ad5555f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chain_config = {\n",
    "    \"llm_model_serving_endpoint_name\": llm_model_serving_endpoint_name,\n",
    "    \"embedding_model\": embedding_model,\n",
    "    \"database_instance_name\": database_instance_name,\n",
    "    \"postgres_database_name\": postgres_database_name,\n",
    "    \"genie_space_id\": genie_space_id,\n",
    "    \"marketing_policy_endpoint\": marketing_policy_endpoint,\n",
    "    \"k\": 3,\n",
    "    \"genie_agent_description\": \"\"\"This Genie agent is designed for marketing analysis, enabling detailed exploration and reporting on audience segments, and individual demographic profiles. The agent integrates campaign data, segment definitions, and audience census profiles. The agent allows to: Understand audience segment definitions and membership, Profiling the campaign audience on demographic dimensions such as age and gender, Developing custom ad-hoc queries for marketing analysts\"\"\",\n",
    "    \"general_assistant_description\": \"The General Assistant synthesizes information from data sources and other agents to provide confident, direct answers. When data is available from previous analysis, present it clearly without unnecessary hedging or disclaimers.\",\n",
    "    \"marketing_policy_agent_description\": \"\"\"This agent is a knowledge assistant to help us adher to marketing policies. Use this agent to validate campaign recommendations, creative content, and data interpretations for policy compliance.\"\"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "247bc64a-7331-4a60-97fe-52d0360bd338",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Chain PY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e706a3d-0b01-44e6-95e1-fcaa22ffaf44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/19 17:41:15 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace URL: e2-demo-field-eng.cloud.databricks.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " View Logged Model at: https://e2-demo-field-eng.cloud.databricks.com/ml/experiments/380662602719503/models/m-3645148db6e44ad7abb32cf10ca49e1a?o=1444828305810485\n",
      "2025/09/19 17:41:17 INFO mlflow.pyfunc: Predicting on input example to validate output\n",
      "2025/09/19 17:41:29 WARNING mlflow.utils.requirements_utils: Detected one or more mismatches between the model's dependencies and the current Python environment:\n",
      " - pydantic (current: 2.11.9, required: pydantic==2.11.7)\n",
      " - tornado (current: 6.4.1, required: tornado==6.3.2)\n",
      "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n",
      "2025/09/19 17:41:29 WARNING mlflow.utils.requirements_utils: Detected one or more mismatches between the model's dependencies and the current Python environment:\n",
      " - pydantic (current: 2.11.9, required: pydantic==2.11.7)\n",
      " - tornado (current: 6.4.1, required: tornado==6.3.2)\n",
      "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n",
      "2025/09/19 17:41:42 INFO mlflow.models.model: Found the following environment variables used during model inference: [DATABRICKS_HOST, DATABRICKS_TOKEN]. Please check if you need to set them when deploying the model. To disable this message, set environment variable `MLFLOW_RECORD_ENV_VARS_IN_MODEL_LOGGING` to `false`.\n",
      "Registered model 'tanner_wendland.default.chat_history_agent_postgres_genie' already exists. Creating a new version of this model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e01c956cc17545608dbe0e33d1db8fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5174f7d7523486ca5b620a98967587a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Created version '55' of model 'tanner_wendland.default.chat_history_agent_postgres_genie': https://e2-demo-field-eng.cloud.databricks.com/explore/data/models/tanner_wendland/default/chat_history_agent_postgres_genie/version/55?o=1444828305810485\n"
     ]
    }
   ],
   "source": [
    "from mlflow.models.resources import (\n",
    "    DatabricksVectorSearchIndex,\n",
    "    DatabricksServingEndpoint,\n",
    "    DatabricksGenieSpace,\n",
    ")\n",
    "\n",
    "chain_file_path = os.path.join(os.getcwd(), \"chain_postgres_genie.py\")\n",
    "if not os.path.exists(chain_file_path):\n",
    "    raise FileNotFoundError(f\"Chain file not found at {chain_file_path}\")\n",
    "\n",
    "workspace_url = spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "print(f\"Workspace URL: {workspace_url}\")\n",
    "os.environ[\"DATABRICKS_HOST\"] = f\"https://{workspace_url}\"\n",
    "os.environ[\"DATABRICKS_TOKEN\"] = (\n",
    "    dbutils.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    ")\n",
    "\n",
    "# Pydantic-based input example - schema will be auto-inferred from type hints\n",
    "# Note: MLflow pyfunc expects List[InputType], so wrap in a list\n",
    "input_example = {\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What was my chat history idea?\"}]\n",
    "}\n",
    "\n",
    "model_config = mlflow.models.ModelConfig(development_config=chain_config)\n",
    "\n",
    "# Use pyfunc log_model with the file path (models-from-code approach)\n",
    "with mlflow.start_run(run_name=\"adtech_chat_history_agent_postgres_genie\"):\n",
    "    logged_chain_info = mlflow.pyfunc.log_model(\n",
    "        python_model=chain_file_path,  # Path to the chain file\n",
    "        model_config=chain_config,\n",
    "        artifact_path=\"chat_agent\",\n",
    "        input_example=input_example,  # Schema auto-inferred from List[ChatRequest] type hint\n",
    "        # Specify resources for automatic authentication passthrough\n",
    "        resources=[\n",
    "            DatabricksServingEndpoint(\n",
    "                endpoint_name=model_config.get(\"llm_model_serving_endpoint_name\")\n",
    "            ),\n",
    "            DatabricksGenieSpace(genie_space_id=model_config.get(\"genie_space_id\")),\n",
    "            DatabricksServingEndpoint(\n",
    "                endpoint_name=model_config.get(\"marketing_policy_endpoint\")\n",
    "            ),\n",
    "        ],\n",
    "        pip_requirements=[\n",
    "            \"mlflow==3.2.0\",\n",
    "            \"databricks-agents==1.4.0\",\n",
    "            \"databricks-langchain==0.7.1\",\n",
    "            \"langchain==0.3.27\",\n",
    "            \"pgvector==0.2.5\",\n",
    "            \"psycopg2-binary==2.9.9\",\n",
    "            \"pydantic==2.11.7\",\n",
    "            \"sqlalchemy==2.0.43\",\n",
    "            \"tornado==6.3.2\",\n",
    "            \"langgraph==0.3.4\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "model_name = \"chat_history_agent_postgres_genie\"\n",
    "MODEL_NAME_FQN = f\"{target_catalog}.{target_schema}.{model_name}\"\n",
    "# Register to UC\n",
    "uc_registered_model_info = mlflow.register_model(\n",
    "    model_uri=logged_chain_info.model_uri, name=MODEL_NAME_FQN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7d1e4f2-7b64-4eb4-97af-4f10df379539",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e499a82902dc48ffa3a2a2e20973ba7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/19 17:41:50 WARNING mlflow.utils.requirements_utils: Detected one or more mismatches between the model's dependencies and the current Python environment:\n",
      " - pydantic (current: 2.11.9, required: pydantic==2.11.7)\n",
      " - tornado (current: 6.4.1, required: tornado==6.3.2)\n",
      "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n"
     ]
    }
   ],
   "source": [
    "AGENT = mlflow.pyfunc.load_model(logged_chain_info.model_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4a985e5-e27b-41c8-a705-82ed8d1ebb9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Test Document Retreival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b3300a0-9b8e-40a5-a8fd-fd3ae12ca393",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test with custom k=10 ===\n",
      "{'messages': [{'role': 'assistant', 'content': 'Based on your chat history, I don\\'t see a specific \"chat history idea\" that you previously mentioned. I can see that you\\'ve had conversations about:\\n\\n1. Marketing campaigns data (Terrific Tacos, Best Burgers, Moving Movie, Fresh Flowers, Super Savings, Cool Car, Happy Dogs)\\n2. Census data related to these campaigns\\n3. Queries about unique people counts and demographic information\\n\\nIf you\\'re referring to a specific idea about chat history functionality or something else that isn\\'t appearing in these results, could you please provide more details about what you\\'re looking for? Perhaps it was from an earlier conversation or you\\'re referring to something specific that needs additional context.', 'name': 'General', 'id': '651044fa-b2c9-4040-b374-c97d77e38c7e'}, {'role': 'assistant', 'content': 'I don\\'t see any previous assistant messages in our conversation history that contain an answer to your question about a \"chat history idea.\" Our conversation has only included exchanges about marketing campaign data, census information, and demographic queries. There are no other assistant messages visible to me that I could reference to answer your question.', 'id': 'run--dd80216b-13e4-4f18-8c99-d9df47855827-0'}]}\n",
      "\n",
      "=== Test with no custom_inputs (should use defaults) ===\n",
      "{'messages': [{'role': 'assistant', 'content': '|    | campaign_name   |   dog_owner_count |\\n|---:|:----------------|------------------:|\\n|  0 | Happy Dogs      |            590877 |', 'name': 'Genie', 'id': '5c16fd52-d6ce-42f6-bc95-f6651bfcdbda'}, {'role': 'assistant', 'content': 'Based on the information provided, the campaign with the most dog owners is \"Happy Dogs\" with 590,877 dog owners.', 'id': 'run--f4a04e57-d73b-43b3-bbf8-52f88ece0a2e-0'}]}\n",
      "\n",
      "=== Test with required user_name but default k ===\n",
      "{'messages': [{'role': 'assistant', 'content': 'Policies regarding tracking dog ownership are as follows:\\n\\n- Dog ownership data may be tracked and used for audience profiling or campaign targeting only if it is lawfully collected.[^B5Ms-1]\\n- Any audience segment based on dog ownership must have a minimum cohort size of 50 individuals (k-anonymity); smaller cohorts must be suppressed.[^B5Ms-1][^B5Ms-2]\\n- Do not combine dog ownership data with other traits to infer health conditions or other sensitive statuses.\\n- Pet ownership (including dog ownership) is not classified as a sensitive attribute, but you must not use it as a proxy for sensitive attributes (such as health, religion, or sexual orientation).[^B5Ms-1]\\n- All targeting must comply with operational guardrails, including avoiding micro-targeting and honoring opt-outs.[^B5Ms-3][^B5Ms-4]\\n- Dog ownership is stored as a census-style attribute and is not considered personally identifiable information (PII) under current policy.[^B5Ms-5]\\n\\nAlways ensure compliance with these policies when tracking or using dog ownership data.\\n\\n[^B5Ms-1]: Pet ownership (dog/cat) may be used when lawfully collected and cohorts remain  50; do not combine with other traits to infer health conditions. - Prohibited: targeting by health, religion, sexual orientation, or proxies suggesting sensitive status. [Policy_02_Privacy_Consent_Policy.pdf](https://e2-demo-field-eng.cloud.databricks.com/ajax-api/2.0/fs/files/Volumes/jai_behl/bricks/policydocs/Policy_02_Privacy_Consent_Policy.pdf)\\n[^B5Ms-2]: Minimum cohort size (k-anonymity) **k  50** for any audience slice surfaced to business users; suppress smaller cohorts. [Policy_17_Data_Dictionary_megacorp_campaigns.pdf](https://e2-demo-field-eng.cloud.databricks.com/ajax-api/2.0/fs/files/Volumes/jai_behl/bricks/policydocs/Policy_17_Data_Dictionary_megacorp_campaigns.pdf)\\n[^B5Ms-3]: Honor opt-outs in all regions; store processing purpose and region for audits. [Policy_04_Regional_Data_Usage_Guidelines.pdf](https://e2-demo-field-eng.cloud.databricks.com/ajax-api/2.0/fs/files/Volumes/jai_behl/bricks/policydocs/Policy_04_Regional_Data_Usage_Guidelines.pdf)\\n[^B5Ms-4]: - Avoid micro-targeting at ZIP+4; ZIP5 allowed only when cohort size  50 and no sensitive attribute joins are present. [Policy_04_Regional_Data_Usage_Guidelines.pdf](https://e2-demo-field-eng.cloud.databricks.com/ajax-api/2.0/fs/files/Volumes/jai_behl/bricks/policydocs/Policy_04_Regional_Data_Usage_Guidelines.pdf)\\n[^B5Ms-5]: Census-style attributes for geography and demographics; includes pet ownership and QSR propensity. [Policy_19_Data_Dictionary_megacorp_audience_census_profile.pdf](https://e2-demo-field-eng.cloud.databricks.com/ajax-api/2.0/fs/files/Volumes/jai_behl/bricks/policydocs/Policy_19_Data_Dictionary_megacorp_audience_census_profile.pdf)', 'name': 'MarketingPolicy', 'id': 'abf57297-46b4-4416-9274-7334a4475bb1'}, {'role': 'assistant', 'content': 'Based on our policies regarding tracking dog ownership:\\n\\n- Dog ownership data can be used for audience profiling or campaign targeting if lawfully collected\\n- Any audience segment based on dog ownership must have a minimum cohort size of 50 individuals (k-anonymity)\\n- Pet ownership (including dogs) is not classified as a sensitive attribute, but cannot be used as a proxy for sensitive attributes\\n- Dog ownership is stored as a census-style attribute and is not considered personally identifiable information (PII)\\n- All targeting must comply with operational guardrails, including avoiding micro-targeting and honoring opt-outs\\n- We should not combine dog ownership data with other traits to infer health conditions or other sensitive statuses\\n\\nThese policies ensure we use dog ownership data responsibly while maintaining privacy standards.', 'id': 'run--f798a9ef-8440-4a52-98d4-dc8ef8c12a15-0'}]}\n"
     ]
    },
    {
     "data": {
      "application/databricks.mlflow.trace": "[\"tr-7166ff915f5b65444e9448cf059d79da\", \"tr-bbeea7e0910a097519e3430398b75432\", \"tr-32b8f465221b824b74a76de314a128ca\"]",
      "text/plain": [
       "[Trace(trace_id=tr-7166ff915f5b65444e9448cf059d79da), Trace(trace_id=tr-bbeea7e0910a097519e3430398b75432), Trace(trace_id=tr-32b8f465221b824b74a76de314a128ca)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test with custom k parameter using pydantic structure\n",
    "# Note: Wrap in list because MLflow pyfunc expects List[ChatRequest]\n",
    "test_input_custom_k = [{\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What was my chat history idea?\"}],\n",
    "    \"custom_inputs\": {\n",
    "        \"filters\": {\"user_name\": \"tanner.wendland@databricks.com\"},\n",
    "        \"k\": 10  # Request 10 records instead of default 3\n",
    "    }\n",
    "}]\n",
    "\n",
    "answer = AGENT.predict(test_input_custom_k)\n",
    "print(\"=== Test with custom k=10 ===\")\n",
    "print(answer)\n",
    "\n",
    "# Test with default k (no custom_inputs provided)\n",
    "test_input_default = [{\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What campaign has the most dog owners?\"}]\n",
    "    # No custom_inputs - should use defaults\n",
    "}]\n",
    "\n",
    "answer_default = AGENT.predict(test_input_default)\n",
    "print(\"\\n=== Test with no custom_inputs (should use defaults) ===\")\n",
    "print(answer_default)\n",
    "\n",
    "# Test with required user_name but no k (should use default k)\n",
    "test_input_required_user = [{\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What policies do we have regarding tracking dog ownership?\"}],\n",
    "    \"custom_inputs\": {\n",
    "        \"filters\": {\"user_name\": \"tanner.wendland@databricks.com\"}\n",
    "        # No k specified - should use default from model_config\n",
    "    }\n",
    "}]\n",
    "\n",
    "answer_required = AGENT.predict(test_input_required_user)\n",
    "print(\"\\n=== Test with required user_name but default k ===\")\n",
    "print(answer_required)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "724b4993-974c-4809-9cb8-93868c0ebd0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Test with Predict Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c74ca68d-6be6-4a23-8caa-0567f9f9f29d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'delta': {'role': 'assistant', 'content': 'Based on searching your chat history, I don\\'t see any specific \"idea\" you mentioned about chat history itself. What I can see is that you\\'ve previously discussed:\\n\\n1. Marketing campaigns data including:\\n   - Six campaigns: Terrific Tacos, Best Burgers, Moving Movie, Fresh Flowers, Super Savings, and Cool Car\\n   - A Happy Dogs campaign was also mentioned\\n   - Statistics about unique people counts across campaigns\\n   - Demographics data like senior male counts\\n\\nIf you were referring to a different idea related to chat history, could you provide more details or context about what specific idea you\\'re trying to recall?', 'name': 'General', 'id': 'e1f93135-bca7-4b8d-9a77-f61be91c7485'}}\n",
      "{'delta': {'role': 'assistant', 'content': 'I don\\'t see any previous assistant messages in our conversation history that contain an answer to your question about a \"chat history idea.\" Our conversation has only included discussion about marketing campaign data (Terrific Tacos, Best Burgers, etc.) and related statistics. There are no other assistant messages visible to me that I could reference to answer your question differently.', 'id': 'run--4407c6a9-e6b3-4ae4-8172-91cc4f822203-0'}}\n"
     ]
    },
    {
     "data": {
      "application/databricks.mlflow.trace": "\"tr-250dc2071a08df0bdbb76204ff635674\"",
      "text/plain": [
       "Trace(trace_id=tr-250dc2071a08df0bdbb76204ff635674)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_input_custom_k = [{\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What was my chat history idea?\"}],\n",
    "    \"custom_inputs\": {\n",
    "        \"filters\": {\"user_name\": \"tanner.wendland@databricks.com\"},\n",
    "        \"k\": 10  # Request 10 records instead of default 3\n",
    "    }\n",
    "}]\n",
    "\n",
    "answer = AGENT.predict_stream(test_input_custom_k)\n",
    "for a in answer:\n",
    "  print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0141fcd2-95ff-4d57-bfa4-3c55a38f6946",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c9639d4-1d86-40b3-87b4-b46c8b7be046",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"secert_scope\", \"adtech-series\", \"Secret Scope\")\n",
    "dbutils.widgets.text(\"secret_key\", \"app-secret\", \"Secret Key\")\n",
    "dbutils.widgets.text(\"permission_group\", \"Adtech Series DB Access Role\", \"Permission Group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93d99ca7-c565-4092-9f28-aff21fcebfa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "secret_scope = dbutils.widgets.get(\"secert_scope\")\n",
    "secret_key = dbutils.widgets.get(\"secret_key\")\n",
    "permission_group = dbutils.widgets.get(\"permission_group\")\n",
    "\n",
    "secret_value = dbutils.secrets.get(scope=secret_scope, key=secret_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbed8a02-2931-4a9f-aa13-e3f2766bf1c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating endpoint chat_history_agent_postgres_genie...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "475196d69ab94592adc0aca0f7f0d9fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mBadRequest\u001b[0m                                Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-4640557381755985>, line 53\u001b[0m\n",
       "\u001b[1;32m     49\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
       "\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating endpoint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mserving_endpoint_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
       "\u001b[0;32m---> 53\u001b[0m deployment \u001b[38;5;241m=\u001b[39m agents\u001b[38;5;241m.\u001b[39mdeploy(\n",
       "\u001b[1;32m     54\u001b[0m     model_name\u001b[38;5;241m=\u001b[39mMODEL_NAME_FQN,\n",
       "\u001b[1;32m     55\u001b[0m     model_version\u001b[38;5;241m=\u001b[39mversion,\n",
       "\u001b[1;32m     56\u001b[0m     scale_to_zero\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
       "\u001b[1;32m     57\u001b[0m     environment_vars\u001b[38;5;241m=\u001b[39m{\n",
       "\u001b[1;32m     58\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATABRICKS_HOST\u001b[39m\u001b[38;5;124m\"\u001b[39m: workspace_url,\n",
       "\u001b[1;32m     59\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATABRICKS_TOKEN\u001b[39m\u001b[38;5;124m\"\u001b[39m: secret_value,\n",
       "\u001b[1;32m     60\u001b[0m     },\n",
       "\u001b[1;32m     61\u001b[0m     workload_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSmall\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
       "\u001b[1;32m     62\u001b[0m     endpoint_name\u001b[38;5;241m=\u001b[39mserving_endpoint_name,\n",
       "\u001b[1;32m     63\u001b[0m )\n",
       "\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Grant permissions to the specified group after endpoint deployment\u001b[39;00m\n",
       "\u001b[1;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGranting permissions to group: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpermission_group\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
       "\n",
       "File \u001b[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-e2041284-9c0d-4fed-84a1-48dafa3a9cac/lib/python3.12/site-packages/databricks/agents/deployments.py:905\u001b[0m, in \u001b[0;36mdeploy\u001b[0;34m(model_name, model_version, scale_to_zero, environment_vars, instance_profile_arn, tags, workload_size, endpoint_name, budget_policy_id, description, **kwargs)\u001b[0m\n",
       "\u001b[1;32m    902\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n",
       "\u001b[1;32m    904\u001b[0m workspace_url \u001b[38;5;241m=\u001b[39m get_workspace_url()\n",
       "\u001b[0;32m--> 905\u001b[0m deployment_info \u001b[38;5;241m=\u001b[39m rest_deploy_chain(\n",
       "\u001b[1;32m    906\u001b[0m     model_name\u001b[38;5;241m=\u001b[39mmodel_name,\n",
       "\u001b[1;32m    907\u001b[0m     model_version\u001b[38;5;241m=\u001b[39mmodel_version,\n",
       "\u001b[1;32m    908\u001b[0m     query_endpoint\u001b[38;5;241m=\u001b[39m_construct_query_endpoint(\n",
       "\u001b[1;32m    909\u001b[0m         workspace_url, endpoint_name, model_name, model_version\n",
       "\u001b[1;32m    910\u001b[0m     ),\n",
       "\u001b[1;32m    911\u001b[0m     endpoint_name\u001b[38;5;241m=\u001b[39mendpoint_name,\n",
       "\u001b[1;32m    912\u001b[0m     served_entity_name\u001b[38;5;241m=\u001b[39m_create_served_model_name(model_name, model_version),\n",
       "\u001b[1;32m    913\u001b[0m     workspace_url\u001b[38;5;241m=\u001b[39mworkspace_url,\n",
       "\u001b[1;32m    914\u001b[0m )\n",
       "\u001b[1;32m    916\u001b[0m \u001b[38;5;66;03m# Create monitor (if unsuccessful, we don't block the deployment)\u001b[39;00m\n",
       "\u001b[1;32m    917\u001b[0m endpoint \u001b[38;5;241m=\u001b[39m _get_endpoint_with_retry(w, deployment_info\u001b[38;5;241m.\u001b[39mendpoint_name)\n",
       "\n",
       "File \u001b[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-e2041284-9c0d-4fed-84a1-48dafa3a9cac/lib/python3.12/site-packages/databricks/agents/client/rest_client.py:94\u001b[0m, in \u001b[0;36mdeploy_chain\u001b[0;34m(model_name, model_version, query_endpoint, endpoint_name, served_entity_name, workspace_url)\u001b[0m\n",
       "\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeploy_chain\u001b[39m(\n",
       "\u001b[1;32m     79\u001b[0m     model_name: \u001b[38;5;28mstr\u001b[39m,\n",
       "\u001b[1;32m     80\u001b[0m     model_version: \u001b[38;5;28mstr\u001b[39m,\n",
       "\u001b[0;32m   (...)\u001b[0m\n",
       "\u001b[1;32m     84\u001b[0m     workspace_url: \u001b[38;5;28mstr\u001b[39m,\n",
       "\u001b[1;32m     85\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Deployment:\n",
       "\u001b[1;32m     86\u001b[0m     request_body \u001b[38;5;241m=\u001b[39m {\n",
       "\u001b[1;32m     87\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_name,\n",
       "\u001b[1;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_version\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_version,\n",
       "\u001b[0;32m   (...)\u001b[0m\n",
       "\u001b[1;32m     92\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworkspace_url\u001b[39m\u001b[38;5;124m\"\u001b[39m: workspace_url,\n",
       "\u001b[1;32m     93\u001b[0m     }\n",
       "\u001b[0;32m---> 94\u001b[0m     response \u001b[38;5;241m=\u001b[39m call_endpoint(\n",
       "\u001b[1;32m     95\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
       "\u001b[1;32m     96\u001b[0m         route\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeployments\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
       "\u001b[1;32m     97\u001b[0m         json_body\u001b[38;5;241m=\u001b[39mrequest_body,\n",
       "\u001b[1;32m     98\u001b[0m     )\n",
       "\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _parse_deploy_chain_response(response)\n",
       "\n",
       "File \u001b[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-e2041284-9c0d-4fed-84a1-48dafa3a9cac/lib/python3.12/site-packages/databricks/agents/utils/rest_utils.py:22\u001b[0m, in \u001b[0;36mcall_endpoint\u001b[0;34m(method, route, json_body)\u001b[0m\n",
       "\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# NOTE: This calls internal Databricks SDK APIs, but MLflow relies on the\u001b[39;00m\n",
       "\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# same ones. See https://github.com/mlflow/mlflow/blob/087e1d56b5690e475571e61b86966d8892eefdf3/mlflow/utils/rest_utils.py#L121-L121\u001b[39;00m\n",
       "\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# TODO: switch to public SDK APIs once available\u001b[39;00m\n",
       "\u001b[1;32m     21\u001b[0m client \u001b[38;5;241m=\u001b[39m WorkspaceClient()\n",
       "\u001b[0;32m---> 22\u001b[0m raw_response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mapi_client\u001b[38;5;241m.\u001b[39mdo(\n",
       "\u001b[1;32m     23\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod, path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/api/2.0/agents/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroute\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, raw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcall_kwargs\n",
       "\u001b[1;32m     24\u001b[0m )\n",
       "\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m raw_response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontents\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39m_response\u001b[38;5;241m.\u001b[39mjson()\n",
       "\n",
       "File \u001b[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-e2041284-9c0d-4fed-84a1-48dafa3a9cac/lib/python3.12/site-packages/databricks/sdk/core.py:85\u001b[0m, in \u001b[0;36mApiClient.do\u001b[0;34m(self, method, path, url, query, headers, body, raw, files, data, auth, response_headers)\u001b[0m\n",
       "\u001b[1;32m     83\u001b[0m     path \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m^/api/2.0/fs/files//\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/api/2.0/fs/files/\u001b[39m\u001b[38;5;124m\"\u001b[39m, path)\n",
       "\u001b[1;32m     84\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cfg\u001b[38;5;241m.\u001b[39mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
       "\u001b[0;32m---> 85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_client\u001b[38;5;241m.\u001b[39mdo(\n",
       "\u001b[1;32m     86\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n",
       "\u001b[1;32m     87\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n",
       "\u001b[1;32m     88\u001b[0m     query\u001b[38;5;241m=\u001b[39mquery,\n",
       "\u001b[1;32m     89\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n",
       "\u001b[1;32m     90\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n",
       "\u001b[1;32m     91\u001b[0m     raw\u001b[38;5;241m=\u001b[39mraw,\n",
       "\u001b[1;32m     92\u001b[0m     files\u001b[38;5;241m=\u001b[39mfiles,\n",
       "\u001b[1;32m     93\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata,\n",
       "\u001b[1;32m     94\u001b[0m     auth\u001b[38;5;241m=\u001b[39mauth,\n",
       "\u001b[1;32m     95\u001b[0m     response_headers\u001b[38;5;241m=\u001b[39mresponse_headers,\n",
       "\u001b[1;32m     96\u001b[0m )\n",
       "\n",
       "File \u001b[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-e2041284-9c0d-4fed-84a1-48dafa3a9cac/lib/python3.12/site-packages/databricks/sdk/_base_client.py:196\u001b[0m, in \u001b[0;36m_BaseClient.do\u001b[0;34m(self, method, url, query, headers, body, raw, files, data, auth, response_headers)\u001b[0m\n",
       "\u001b[1;32m    193\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetry disabled for non-seekable stream: type=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
       "\u001b[1;32m    194\u001b[0m     call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_perform\n",
       "\u001b[0;32m--> 196\u001b[0m response \u001b[38;5;241m=\u001b[39m call(\n",
       "\u001b[1;32m    197\u001b[0m     method,\n",
       "\u001b[1;32m    198\u001b[0m     url,\n",
       "\u001b[1;32m    199\u001b[0m     query\u001b[38;5;241m=\u001b[39mquery,\n",
       "\u001b[1;32m    200\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n",
       "\u001b[1;32m    201\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n",
       "\u001b[1;32m    202\u001b[0m     raw\u001b[38;5;241m=\u001b[39mraw,\n",
       "\u001b[1;32m    203\u001b[0m     files\u001b[38;5;241m=\u001b[39mfiles,\n",
       "\u001b[1;32m    204\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata,\n",
       "\u001b[1;32m    205\u001b[0m     auth\u001b[38;5;241m=\u001b[39mauth,\n",
       "\u001b[1;32m    206\u001b[0m )\n",
       "\u001b[1;32m    208\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n",
       "\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m header \u001b[38;5;129;01min\u001b[39;00m response_headers \u001b[38;5;28;01mif\u001b[39;00m response_headers \u001b[38;5;28;01melse\u001b[39;00m []:\n",
       "\n",
       "File \u001b[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-e2041284-9c0d-4fed-84a1-48dafa3a9cac/lib/python3.12/site-packages/databricks/sdk/retries.py:57\u001b[0m, in \u001b[0;36mretried.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m     53\u001b[0m         retry_reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(err)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is allowed to retry\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
       "\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retry_reason \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# raise if exception is not retryable\u001b[39;00m\n",
       "\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n",
       "\u001b[1;32m     59\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mretry_reason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (sleeping ~\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msleep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
       "\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m before_retry:\n",
       "\n",
       "File \u001b[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-e2041284-9c0d-4fed-84a1-48dafa3a9cac/lib/python3.12/site-packages/databricks/sdk/retries.py:36\u001b[0m, in \u001b[0;36mretried.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m clock\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m<\u001b[39m deadline:\n",
       "\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "\u001b[0;32m---> 36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
       "\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
       "\u001b[1;32m     38\u001b[0m         last_err \u001b[38;5;241m=\u001b[39m err\n",
       "\n",
       "File \u001b[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-e2041284-9c0d-4fed-84a1-48dafa3a9cac/lib/python3.12/site-packages/databricks/sdk/_base_client.py:298\u001b[0m, in \u001b[0;36m_BaseClient._perform\u001b[0;34m(self, method, url, query, headers, body, raw, files, data, auth)\u001b[0m\n",
       "\u001b[1;32m    296\u001b[0m error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_parser\u001b[38;5;241m.\u001b[39mget_api_error(response)\n",
       "\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "\u001b[0;32m--> 298\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
       "\n",
       "\u001b[0;31mBadRequest\u001b[0m: The quota for the number of INTERNAL_HIDDEN jobs has been reached. The current quota is 1500."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "BadRequest",
        "evalue": "The quota for the number of INTERNAL_HIDDEN jobs has been reached. The current quota is 1500."
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>BadRequest</span>: The quota for the number of INTERNAL_HIDDEN jobs has been reached. The current quota is 1500.\n404 Not Found: RESOURCE_DOES_NOT_EXIST: Path (/Users/tanner.wendland@databricks.com/.bundle/adtech_series_app/dev/files/agents/src/models:) doesn't exist.\n\nRequest ID: df027873-a645-4b1c-84d5-b93719ad312c\n[Trace ID: 00-b2afa9e4b52f6bae36132a2a5fa601d9-416b6ebe0f6a1454-00]"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[0;31mBadRequest\u001b[0m                                Traceback (most recent call last)",
        "File \u001b[0;32m<command-4640557381755985>, line 53\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating endpoint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mserving_endpoint_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 53\u001b[0m deployment \u001b[38;5;241m=\u001b[39m agents\u001b[38;5;241m.\u001b[39mdeploy(\n\u001b[1;32m     54\u001b[0m     model_name\u001b[38;5;241m=\u001b[39mMODEL_NAME_FQN,\n\u001b[1;32m     55\u001b[0m     model_version\u001b[38;5;241m=\u001b[39mversion,\n\u001b[1;32m     56\u001b[0m     scale_to_zero\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     57\u001b[0m     environment_vars\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATABRICKS_HOST\u001b[39m\u001b[38;5;124m\"\u001b[39m: workspace_url,\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATABRICKS_TOKEN\u001b[39m\u001b[38;5;124m\"\u001b[39m: secret_value,\n\u001b[1;32m     60\u001b[0m     },\n\u001b[1;32m     61\u001b[0m     workload_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSmall\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     62\u001b[0m     endpoint_name\u001b[38;5;241m=\u001b[39mserving_endpoint_name,\n\u001b[1;32m     63\u001b[0m )\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Grant permissions to the specified group after endpoint deployment\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGranting permissions to group: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpermission_group\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
        "File \u001b[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-e2041284-9c0d-4fed-84a1-48dafa3a9cac/lib/python3.12/site-packages/databricks/agents/deployments.py:905\u001b[0m, in \u001b[0;36mdeploy\u001b[0;34m(model_name, model_version, scale_to_zero, environment_vars, instance_profile_arn, tags, workload_size, endpoint_name, budget_policy_id, description, **kwargs)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    904\u001b[0m workspace_url \u001b[38;5;241m=\u001b[39m get_workspace_url()\n\u001b[0;32m--> 905\u001b[0m deployment_info \u001b[38;5;241m=\u001b[39m rest_deploy_chain(\n\u001b[1;32m    906\u001b[0m     model_name\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[1;32m    907\u001b[0m     model_version\u001b[38;5;241m=\u001b[39mmodel_version,\n\u001b[1;32m    908\u001b[0m     query_endpoint\u001b[38;5;241m=\u001b[39m_construct_query_endpoint(\n\u001b[1;32m    909\u001b[0m         workspace_url, endpoint_name, model_name, model_version\n\u001b[1;32m    910\u001b[0m     ),\n\u001b[1;32m    911\u001b[0m     endpoint_name\u001b[38;5;241m=\u001b[39mendpoint_name,\n\u001b[1;32m    912\u001b[0m     served_entity_name\u001b[38;5;241m=\u001b[39m_create_served_model_name(model_name, model_version),\n\u001b[1;32m    913\u001b[0m     workspace_url\u001b[38;5;241m=\u001b[39mworkspace_url,\n\u001b[1;32m    914\u001b[0m )\n\u001b[1;32m    916\u001b[0m \u001b[38;5;66;03m# Create monitor (if unsuccessful, we don't block the deployment)\u001b[39;00m\n\u001b[1;32m    917\u001b[0m endpoint \u001b[38;5;241m=\u001b[39m _get_endpoint_with_retry(w, deployment_info\u001b[38;5;241m.\u001b[39mendpoint_name)\n",
        "File \u001b[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-e2041284-9c0d-4fed-84a1-48dafa3a9cac/lib/python3.12/site-packages/databricks/agents/client/rest_client.py:94\u001b[0m, in \u001b[0;36mdeploy_chain\u001b[0;34m(model_name, model_version, query_endpoint, endpoint_name, served_entity_name, workspace_url)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeploy_chain\u001b[39m(\n\u001b[1;32m     79\u001b[0m     model_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     80\u001b[0m     model_version: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m     workspace_url: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     85\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Deployment:\n\u001b[1;32m     86\u001b[0m     request_body \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_name,\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_version\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_version,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworkspace_url\u001b[39m\u001b[38;5;124m\"\u001b[39m: workspace_url,\n\u001b[1;32m     93\u001b[0m     }\n\u001b[0;32m---> 94\u001b[0m     response \u001b[38;5;241m=\u001b[39m call_endpoint(\n\u001b[1;32m     95\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     96\u001b[0m         route\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeployments\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     97\u001b[0m         json_body\u001b[38;5;241m=\u001b[39mrequest_body,\n\u001b[1;32m     98\u001b[0m     )\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _parse_deploy_chain_response(response)\n",
        "File \u001b[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-e2041284-9c0d-4fed-84a1-48dafa3a9cac/lib/python3.12/site-packages/databricks/agents/utils/rest_utils.py:22\u001b[0m, in \u001b[0;36mcall_endpoint\u001b[0;34m(method, route, json_body)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# NOTE: This calls internal Databricks SDK APIs, but MLflow relies on the\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# same ones. See https://github.com/mlflow/mlflow/blob/087e1d56b5690e475571e61b86966d8892eefdf3/mlflow/utils/rest_utils.py#L121-L121\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# TODO: switch to public SDK APIs once available\u001b[39;00m\n\u001b[1;32m     21\u001b[0m client \u001b[38;5;241m=\u001b[39m WorkspaceClient()\n\u001b[0;32m---> 22\u001b[0m raw_response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mapi_client\u001b[38;5;241m.\u001b[39mdo(\n\u001b[1;32m     23\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod, path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/api/2.0/agents/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroute\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, raw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcall_kwargs\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m raw_response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontents\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39m_response\u001b[38;5;241m.\u001b[39mjson()\n",
        "File \u001b[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-e2041284-9c0d-4fed-84a1-48dafa3a9cac/lib/python3.12/site-packages/databricks/sdk/core.py:85\u001b[0m, in \u001b[0;36mApiClient.do\u001b[0;34m(self, method, path, url, query, headers, body, raw, files, data, auth, response_headers)\u001b[0m\n\u001b[1;32m     83\u001b[0m     path \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m^/api/2.0/fs/files//\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/api/2.0/fs/files/\u001b[39m\u001b[38;5;124m\"\u001b[39m, path)\n\u001b[1;32m     84\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cfg\u001b[38;5;241m.\u001b[39mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_client\u001b[38;5;241m.\u001b[39mdo(\n\u001b[1;32m     86\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m     87\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m     88\u001b[0m     query\u001b[38;5;241m=\u001b[39mquery,\n\u001b[1;32m     89\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m     90\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m     91\u001b[0m     raw\u001b[38;5;241m=\u001b[39mraw,\n\u001b[1;32m     92\u001b[0m     files\u001b[38;5;241m=\u001b[39mfiles,\n\u001b[1;32m     93\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m     94\u001b[0m     auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[1;32m     95\u001b[0m     response_headers\u001b[38;5;241m=\u001b[39mresponse_headers,\n\u001b[1;32m     96\u001b[0m )\n",
        "File \u001b[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-e2041284-9c0d-4fed-84a1-48dafa3a9cac/lib/python3.12/site-packages/databricks/sdk/_base_client.py:196\u001b[0m, in \u001b[0;36m_BaseClient.do\u001b[0;34m(self, method, url, query, headers, body, raw, files, data, auth, response_headers)\u001b[0m\n\u001b[1;32m    193\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetry disabled for non-seekable stream: type=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    194\u001b[0m     call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_perform\n\u001b[0;32m--> 196\u001b[0m response \u001b[38;5;241m=\u001b[39m call(\n\u001b[1;32m    197\u001b[0m     method,\n\u001b[1;32m    198\u001b[0m     url,\n\u001b[1;32m    199\u001b[0m     query\u001b[38;5;241m=\u001b[39mquery,\n\u001b[1;32m    200\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    201\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    202\u001b[0m     raw\u001b[38;5;241m=\u001b[39mraw,\n\u001b[1;32m    203\u001b[0m     files\u001b[38;5;241m=\u001b[39mfiles,\n\u001b[1;32m    204\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m    205\u001b[0m     auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[1;32m    206\u001b[0m )\n\u001b[1;32m    208\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m header \u001b[38;5;129;01min\u001b[39;00m response_headers \u001b[38;5;28;01mif\u001b[39;00m response_headers \u001b[38;5;28;01melse\u001b[39;00m []:\n",
        "File \u001b[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-e2041284-9c0d-4fed-84a1-48dafa3a9cac/lib/python3.12/site-packages/databricks/sdk/retries.py:57\u001b[0m, in \u001b[0;36mretried.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m         retry_reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(err)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is allowed to retry\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retry_reason \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# raise if exception is not retryable\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m     59\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mretry_reason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (sleeping ~\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msleep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m before_retry:\n",
        "File \u001b[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-e2041284-9c0d-4fed-84a1-48dafa3a9cac/lib/python3.12/site-packages/databricks/sdk/retries.py:36\u001b[0m, in \u001b[0;36mretried.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m clock\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m<\u001b[39m deadline:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m     38\u001b[0m         last_err \u001b[38;5;241m=\u001b[39m err\n",
        "File \u001b[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-e2041284-9c0d-4fed-84a1-48dafa3a9cac/lib/python3.12/site-packages/databricks/sdk/_base_client.py:298\u001b[0m, in \u001b[0;36m_BaseClient._perform\u001b[0;34m(self, method, url, query, headers, body, raw, files, data, auth)\u001b[0m\n\u001b[1;32m    296\u001b[0m error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_parser\u001b[38;5;241m.\u001b[39mget_api_error(response)\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 298\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
        "\u001b[0;31mBadRequest\u001b[0m: The quota for the number of INTERNAL_HIDDEN jobs has been reached. The current quota is 1500."
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import (\n",
    "    EndpointCoreConfigInput,\n",
    "    ServedEntityInput,\n",
    "    AiGatewayConfig,\n",
    "    ServingEndpointAccessControlRequest,\n",
    "    ServingEndpointPermissionLevel,\n",
    ")\n",
    "from databricks import agents\n",
    "\n",
    "workspace_client = WorkspaceClient()\n",
    "\n",
    "version = uc_registered_model_info.version\n",
    "serving_endpoint_name = model_name.replace(\".\", \"-\")\n",
    "\n",
    "workspace_url = spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "\n",
    "config = {\n",
    "    \"served_entities\": [\n",
    "        {\n",
    "            \"name\": serving_endpoint_name,\n",
    "            \"entity_name\": MODEL_NAME_FQN,\n",
    "            \"entity_version\": version,\n",
    "            \"workload_size\": \"Small\",\n",
    "            \"scale_to_zero_enabled\": True,\n",
    "            \"environment_vars\": {\n",
    "                \"DATABRICKS_HOST\": workspace_url,\n",
    "                \"DATABRICKS_TOKEN\": secret_value,\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "ai_gateway_config = {\n",
    "    \"inference_table_config\": {\n",
    "        \"enabled\": True,\n",
    "        \"catalog_name\": target_catalog,\n",
    "        \"schema_name\": target_schema,\n",
    "        \"table_name\": \"chat_history_agent_postgres_genie_inference\",\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def does_endpoint_exists(endpoint_name):\n",
    "    try:\n",
    "        workspace_client.serving_endpoints.get(endpoint_name)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "print(f\"Creating endpoint {serving_endpoint_name}...\")\n",
    "deployment = agents.deploy(\n",
    "    model_name=MODEL_NAME_FQN,\n",
    "    model_version=version,\n",
    "    scale_to_zero=True,\n",
    "    environment_vars={\n",
    "        \"DATABRICKS_HOST\": workspace_url,\n",
    "        \"DATABRICKS_TOKEN\": secret_value,\n",
    "    },\n",
    "    workload_size=\"Small\",\n",
    "    endpoint_name=serving_endpoint_name,\n",
    ")\n",
    "\n",
    "# Grant permissions to the specified group after endpoint deployment\n",
    "print(f\"Granting permissions to group: {permission_group}\")\n",
    "try:\n",
    "    agents.set_permissions(model_name=MODEL_NAME_FQN, users=[permission_group], permission_level=\"CAN_QUERY\")\n",
    "    print(f\"Successfully granted CAN_QUERY permission to group: {permission_group}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to grant permissions to group {permission_group}: {str(e)}\")\n",
    "    print(\"Please manually grant permissions to the endpoint if needed.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "chat-history-agent-pg-genie",
   "widgets": {
    "database_instance_name": {
     "currentValue": "adtech-series-db-do-not-delete",
     "nuid": "5935864b-337f-4cea-978f-00fcf2a24646",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "adtech-series-do-not-delete",
      "label": null,
      "name": "database_instance_name",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "adtech-series-do-not-delete",
      "label": null,
      "name": "database_instance_name",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "embedding_model": {
     "currentValue": "databricks-gte-large-en",
     "nuid": "96111585-d614-405c-960e-ae589ccc89fc",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "databricks-gte-large-en",
      "label": null,
      "name": "embedding_model",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "databricks-gte-large-en",
      "label": null,
      "name": "embedding_model",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "genie_space_id": {
     "currentValue": "01f07c4ba44615aab8989b10e0a95420",
     "nuid": "ac774ec2-f551-4a54-94f9-5c547df32515",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "01f07c4ba44615aab8989b10e0a95420",
      "label": null,
      "name": "genie_space_id",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "01f07c4ba44615aab8989b10e0a95420",
      "label": null,
      "name": "genie_space_id",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "llm_model_serving_endpoint_name": {
     "currentValue": "databricks-claude-3-7-sonnet",
     "nuid": "4e5299b7-86bb-4511-bd44-8d76be56893e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "databricks-claude-3-7-sonnet",
      "label": null,
      "name": "llm_model_serving_endpoint_name",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "databricks-claude-3-7-sonnet",
      "label": null,
      "name": "llm_model_serving_endpoint_name",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "marketing_policy_endpoint": {
     "currentValue": "ka-c9af3fe4-endpoint",
     "nuid": "b06e2c94-609c-4313-ae50-ef5ab1d80527",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "ka-c9af3fe4-endpoint",
      "label": null,
      "name": "marketing_policy_endpoint",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "ka-c9af3fe4-endpoint",
      "label": null,
      "name": "marketing_policy_endpoint",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "permission_group": {
     "currentValue": "Adtech Series DB Access Role",
     "nuid": "fc8d7e9a-8c23-41e1-affe-706c0f79c5e7",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "Adtech Series DB Access Role",
      "label": "Permission Group",
      "name": "permission_group",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "Adtech Series DB Access Role",
      "label": "Permission Group",
      "name": "permission_group",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "postgres_database_name": {
     "currentValue": "databricks_postgres",
     "nuid": "e75a0daa-dd0b-412d-b393-29edb1e3b1f1",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "databricks_postgres",
      "label": null,
      "name": "postgres_database_name",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "databricks_postgres",
      "label": null,
      "name": "postgres_database_name",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "secert_scope": {
     "currentValue": "adtech-series",
     "nuid": "e227bce8-9119-4ce0-9bd1-7bb37e315a10",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "adtech-series",
      "label": "Secret Scope",
      "name": "secert_scope",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "adtech-series",
      "label": "Secret Scope",
      "name": "secert_scope",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "secret_key": {
     "currentValue": "app-secret",
     "nuid": "90c15fd3-85b5-4532-9fb1-cd73d8f32360",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "app-secret",
      "label": "Secret Key",
      "name": "secret_key",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "app-secret",
      "label": "Secret Key",
      "name": "secret_key",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "target_catalog": {
     "currentValue": "tanner_wendland",
     "nuid": "9ce71cf8-98af-4e13-8cc0-71dc8c6cf387",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "tanner_wendland",
      "label": null,
      "name": "target_catalog",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "tanner_wendland",
      "label": null,
      "name": "target_catalog",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "target_schema": {
     "currentValue": "default",
     "nuid": "eb61b685-72df-4bd5-bf4e-80e589c9adb5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "default",
      "label": null,
      "name": "target_schema",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "default",
      "label": null,
      "name": "target_schema",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
